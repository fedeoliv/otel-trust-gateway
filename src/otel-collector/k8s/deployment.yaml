---
# ConfigMap for OTel Collector Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: otelcol-config
  namespace: observability
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          http:
            endpoint: 0.0.0.0:4318
          grpc:
            endpoint: 0.0.0.0:4317

    processors:
      # Memory Limiter - Set to 80% of pod memory limit for safety margin
      memory_limiter:
        check_interval: 1s
        limit_mib: 410  # 80% of 512Mi pod limit
        spike_limit_mib: 128  # Allow temporary spikes

      # Trust Gateway - Custom validation
      trustgateway:
        required_headers:
          - "X-App-Token"
        valid_api_keys:
          - "mobile-app-secret-key-123"
          - "mobile-app-secret-key-456"

      # Batch Processor - Optimized for mobile apps
      batch:
        timeout: 5s
        send_batch_size: 100

    exporters:
      debug:
        verbosity: detailed

      # Example: Send to external system
      # otlp:
      #   endpoint: "external-collector.observability.svc.cluster.local:4317"
      #   tls:
      #     insecure: true

    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
      pprof:
        endpoint: 0.0.0.0:1777
      zpages:
        endpoint: 0.0.0.0:55679

    service:
      extensions: [health_check, pprof, zpages]
      telemetry:
        logs:
          level: info
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, trustgateway, batch]
          exporters: [debug]
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, trustgateway, batch]
          exporters: [debug]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, trustgateway, batch]
          exporters: [debug]

---
# Deployment with High Availability
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otelcol-custom
  namespace: observability
  labels:
    app: otelcol-custom
spec:
  # High Availability: Run 3 replicas for redundancy
  replicas: 3
  
  # Rolling update strategy to avoid downtime
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # Allow 1 extra pod during updates
      maxUnavailable: 1  # Allow 1 pod to be unavailable during updates
  
  selector:
    matchLabels:
      app: otelcol-custom
  
  template:
    metadata:
      labels:
        app: otelcol-custom
      annotations:
        # Force restart on config changes
        checksum/config: "{{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}"
    
    spec:
      # Spread pods across availability zones for resilience
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: otelcol-custom
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app: otelcol-custom
      
      # Anti-affinity: Don't schedule multiple pods on same node
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: otelcol-custom
                topologyKey: kubernetes.io/hostname
      
      containers:
        - name: otelcol
          image: otelcol-custom:latest
          imagePullPolicy: IfNotPresent
          
          ports:
            - name: otlp-grpc
              containerPort: 4317
              protocol: TCP
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
            - name: healthcheck
              containerPort: 13133
              protocol: TCP
            - name: pprof
              containerPort: 1777
              protocol: TCP
            - name: zpages
              containerPort: 55679
              protocol: TCP
          
          # Resource limits - Critical for memory_limiter to work properly
          resources:
            requests:
              cpu: 200m
              memory: 256Mi
            limits:
              cpu: 1000m
              memory: 512Mi  # Set memory_limiter to 80% of this (410Mi)
          
          # Liveness probe - Restart if collector becomes unhealthy
          livenessProbe:
            httpGet:
              path: /
              port: healthcheck
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          
          # Readiness probe - Remove from load balancer if not ready
          readinessProbe:
            httpGet:
              path: /
              port: healthcheck
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 2
          
          # Startup probe - Give time for slow startup
          startupProbe:
            httpGet:
              path: /
              port: healthcheck
            initialDelaySeconds: 5
            periodSeconds: 5
            failureThreshold: 12  # 60 seconds total
          
          # Mount configuration
          volumeMounts:
            - name: config
              mountPath: /etc/otelcol
          
          # Use config from mounted volume
          args:
            - "--config=/etc/otelcol/config.yaml"
          
          # Security context
          securityContext:
            runAsNonRoot: true
            runAsUser: 10001
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
      
      volumes:
        - name: config
          configMap:
            name: otelcol-config
      
      # Graceful shutdown
      terminationGracePeriodSeconds: 30

---
# Service with Load Balancing
apiVersion: v1
kind: Service
metadata:
  name: otelcol-custom
  namespace: observability
  labels:
    app: otelcol-custom
spec:
  type: ClusterIP
  # Session affinity for better performance (optional)
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800  # 3 hours
  
  ports:
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    - name: healthcheck
      port: 13133
      targetPort: 13133
      protocol: TCP
  
  selector:
    app: otelcol-custom

---
# Horizontal Pod Autoscaler - Scale based on load
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: otelcol-custom-hpa
  namespace: observability
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: otelcol-custom
  
  minReplicas: 3
  maxReplicas: 10
  
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 minutes before scaling down
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0  # Scale up immediately
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 2
          periodSeconds: 30
  
  metrics:
    # Scale based on memory usage
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 70  # Scale up when avg memory > 70%
    
    # Scale based on CPU usage
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70  # Scale up when avg CPU > 70%

---
# Pod Disruption Budget - Ensure availability during maintenance
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: otelcol-custom-pdb
  namespace: observability
spec:
  minAvailable: 2  # Always keep at least 2 pods running
  selector:
    matchLabels:
      app: otelcol-custom

---
# Service Monitor for Prometheus (optional)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: otelcol-custom
  namespace: observability
  labels:
    app: otelcol-custom
spec:
  selector:
    matchLabels:
      app: otelcol-custom
  endpoints:
    - port: healthcheck
      path: /metrics
      interval: 30s
